---
title: "Workshop C3: Diagnostics and Transformations"
output:
  html_document:
    toc: yes
---

```{r}
library(tidyverse)
```

## Exercise 1: Visualising the model with a prediction band

`visreg` provides us an efficient routine to visualise our linear model. However, it only produces the line with a confidence band. We need to do a bit more work to visualising our model with a prediction band. 

1. Load the package `datarium` and the data `marketing`. Fit the linear model, and save the result in the object `lm.youtube` again. Then, sweep your fitted result by the package `broom` and get the tidy version `lm.youtube`.

    ```{r}
    library(datarium)
    library(broom)
    data(`marketing`)
    lm.youtube <- lm(sales ~ youtube, data=marketing)
    lm.youtube.fit <- augment(lm.youtube)
    ```

2. Get all prediction intervals for each observed `youtube` by calling `predict()` without supplying `new.data`, and save the result in `lm.youtube.pred`. We can then bind `lm.youtube.pred` with `lm.youtube.fit` by column with the R function `cbind()`, aka colunm bind.

    ```{r}
    lm.youtube.pred <- predict(lm.youtube, interval='prediction')
    lm.youtube.fit.pred <- lm.youtube.fit |> cbind(lm.youtube.pred) |> select(-fit) |> tibble()
    lm.youtube.fit.pred
    ```

    Here we further remove the duplicate column `fit`.

3. First, visualise your linear model with the fitted line and the scatter plot. You can then add the prediction band by the R function `geom_ribbon()` with `aes(ymin = lwr, ymax = upr)` which defines a shaded region bounded by the lower limits and upper limits of the prediction intervals in your plot. You may need to adjust `fill` and `alpha` in `geom_ribbon()` to get a nicer plot.

    ```{r}
    lm.youtube.fit.pred |> ggplot(aes(x = youtube ,y=sales)) +
        geom_point() +
        geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "grey70", alpha=0.5) +
        geom_smooth(method='lm')
    ```
    
4. The default prediction interval in R is a 95% prediction interval which means around 95% observations shall fall into the bands. *Check the plot in Step 3 and count the points falling outside the prediction band. Add one or two comments on your findings.*

5. Actually, the prediction band we plotted is a **pointwise** 95% prediction interval. The term **pointwise** means that, each individual observation falls into the corresponding prediction interval with a probability 95%. *Try to split `youtube` into two groups `youtube<200` and `youtube>200` and count the points falling in the prediction band for each group. Add one or two comments on your findings.*


## Exercise 2: Model diagnostics

As we can see from Exercise 1, the prediction interval tends to be too wide when `youtube` is large and too narrow when `youtube` is small. A similar issue also arises when we try to predict the sales given zero budget in Exercise 3 of Lab C2. The prediction at zero budget may significantly overestimate the actual sales.

These issues allude to a critical point: does our linear model provide a good fit to our data?

Of course, we can always get some clues from the scatter plot and smoothed curve for `sales` against `youtube`. We can even conclude that the fitted line fails to capture a significant portion of uncertainties in our data. So a linear model may not be an adequate model for the relationship between `sales` and `youtube`.

The next question is how to identify and quantify the inadequacy of our linear model? 

The visualisation of model identifies some ill-posed patterns in our model but it fails to identify the root cause of the problem. $R^2$ is a simple indicator but it won't capture the issues on prediction discussed above. 

What we need are some diagnostic tools to check the fitness of our linear model on the data set and identify the crux in our fitted model. However, before we carry out the examination on our fitted linear model, we need to know when a fitted linear model will behave well. 

To ensure the overall good performance of a fitted linear model, we need our data satisfy a few conditions, i.e. four assumptions as follows 

  * **L**inearity

      Residuals don't depend on $x$. The trend is correctly modelled as a line. A line won't fit an exponential trend. 

  * **I**ndependence

      Residuals don't depend on each other. If residuals are dependent, we can expect that one residual may contribute some information to another residual and vice versa. 

  * **N**ormality

      Residuals are distributed normally. Normality ensures that the least square estimation will catch the best possible line.  

  * **E**qual variance

      Residuals have constant variance. The variation doesn't change as we move along the trend.
  
with Linearity being the most important. 

1. Firstly we'll be looking at linearity and equal variance, both of which can be assessed using a plot of **the residuals versus the fitted value**.

    If linearity holds, we'd expect a plot of residuals vs fitted value to show no trend - the points should be scattered fairly constantly above and below the line - in particular we don't want to see a curve.

    If equal variance holds, we'd expect the scatter of points around the trend to be constant as the fitted value changes. You want it to be relatively even, and in particular not increasing from left to right (i.e. not spreading out).

    We can demonstrate this idea by using the stochastic simulation. An example of a good plot (left) and bad plot (right) is shown below for two artificial data sets, i.e., linear and exponential. 

```{r,  fig.width=8, fig.height=3,}
    set.seed(2020)
    n <- 50
    ab <- c(0,2)
    demo <- tibble(x=(1:n)/n, e= rnorm(n, sd=0.1)) |> mutate (y=ab[1]+ab[2]*x+e) |> 
      mutate (y2=exp(ab[1]+ab[2]*x+e))
    demo |> select(x, y, y2) |> gather(key = "Group", value = "Y", -x) |>
      ggplot(aes(x=x,y=Y,col=Group)) + geom_point()
    g1 <- lm(y~x,data=demo) |> augment() |> 
      ggplot(aes(x=.fitted,y=.resid)) + geom_point() +geom_smooth()+geom_hline(yintercept=0)
    g2 <- lm(y2~x,data=demo) |> augment() |> 
      ggplot(aes(x=.fitted,y=.resid)) + geom_point() +geom_smooth()+geom_hline(yintercept=0)
    library(patchwork)
    g1+g2 
```

    *Compare the scatter plots and the plots of residuals versus fitted values. Think about the reason why we include horizontal lines at $y=0$ and smoothed curves with confidence bands in the residual plots.*

    You can see more examples using the interactive found here:
 
    https://shiny.massey.ac.nz/jcmarsha/linearity

    *You can of course modify some parameters in the above R code chunk and even change the exponential function to some other functions. A possible choice is to change `ab <- c(0,2)` to `ab <- c(0,0.5)`.*

2. Let's see how well our model for sales does by producing the diagnostic plot for the linear model you fit above using the following. 
    
    ```{r}
    lm.youtube.fit |> ggplot(aes(x=.fitted,y=.resid)) +
      geom_point()+geom_smooth()+geom_hline(yintercept=0)
    ```

    *Take a good look at the plot. Do you think linearity and equal variance hold? Add some notes about each assumption to your notebook.*
    
3. In addition to the residuals vs fits plot, we have another tool, i.e. **the scale-location plot**, to check linearity and equal variance. The plot can be generated by using the following R code chunk:
    
    ```{r}
    lm.youtube.fit |> mutate(.root.abs.std.resid=sqrt(abs(.std.resid))) |>
      ggplot(aes(x=.fitted,y=.root.abs.std.resid)) + geom_point()+geom_smooth()+geom_hline(yintercept=0)
    ```
    
    Similar to the residual versus fits plot, the scale-Location plot shows whether residuals are spread equally along the ranges of input variables (predictor). The assumption of equal variance (**homoscedasticity**) could also be checked with this plot. If we see a horizontal line with randomly spread points, it means that the model is good.
    
    *Take a good look at the plot. Do you think linearity and equal variance hold? Add some notes about each assumption to your notebook.*
    
    In the scale-location plot we still use the fitted values as $x$ coordinates. But another quantity - the square root of absolute value of **standardised residuals**- is used as $y$ coordinates. Standardised residuals are obtained by dividing the residuals by the standard error of residuals as $s_i=e_i/sd_{res}$. An advantange of standardised residuals is that both the variance and standard error are just 1 for this sequence. 
    
    The standardised residuals can be readily obtained from the fitted model by the R function `rstandard()`. Otherwise, if you have carefully read the tibble from `augment()` and the R code chunk for the scale-location plot, you should have found the standardised residuals `.std.resid` right following `.resid`.
    
    The operation of dividing a sequence by its standard error is called **standardisation** in statistics. It is frequently used in statistics as the data on different scales can be handle with similar measures after the standardisaton. 
    
    **Optional Challenge: Modify the R code chunk in Step 1 and perform a simulation study on the scale-location plot.**

4. The residuals vs fits plot and scale-location plot are usually sufficient for us to tell the issues on linearity and equal variance. Our next task is to verify the normality. Recall the simulation study in Lab 2 where we make a scatter plot of the residuals versus true random errors which can be approximated by the line $y=x$. Even if we can't observe the true random error in a real problem, we can still make a similar plot called **quantile-quantile plot**, aka **Q-Q plot**, via `geom_qq()` and `geom_qq_line` as follows. 

    ```{r}
    lm.youtube.fit |> ggplot(aes(sample=.std.resid)) + geom_qq(alpha=0.3) +  geom_qq_line(color='red')
    ```

    We are using the standardised residuals again. `geom_qq()` adds the black points while `geom_qq_line()` dispicts the red line. If most points follow the red line, we can feel free to assume the normality of the residuals. However, one must confirm the linearity and equal variance first before thinking about the normality. 
    
    Of course, we need to know the appearance of a good Q-Q plot before we make any solid judgement. The normality assumption says the residuals follow normal distribution and the standardised residuals has zero mean and unit variance. We can then simulate a standard normal sample with zero mean and unit variance and check its Q-Q plot as follows.
    
    ```{r}
    set.seed(2020)
    tibble(e=rnorm(100)) |> ggplot(aes(sample=e)) + geom_qq(alpha=0.3) +  geom_qq_line(color='red')
    ```
    
    *Take a good look at the above two plots. Do you think the normality hold? Add some notes about the assumption to your notebook.*

5. The last thing to check is slightly different from the above assumptions. We will look at the **outliers** in linear model. Just like we observe some distant points in a boxplot for one variable, we can have some outliers for $x$ and $y$. The trick thing is that, even both $x$ and $y$ look good in their own boxplots, their joint effort may push the point away from the main body in a scatter plot. A possible pitfall in fitting a linear model is that the linear model found by least square method is not very robust against outliers. Only a few outliers can distort the point estimates significantly. So it is essential to spot those bad guys hidden in our data. 

    Of course, even a scatter plot can help us identify the outliers. If we want some more quantitative measurements, there is a specific measure called **Cook's distance**, or just Cook's $D$, for each data point to measure its influence on the regression line. A large Cook's distance suggests that this point may be an outlier which has a big influence on the whole regression line. 
    This peice of informaiont is readily collected by `augment()` in the column `.cooksd`. The following R code chunk produces a plot of Cook's distance against the index. 
    
    ```{r}
    lm.youtube.fit |> mutate(.index=1:n()) |> ggplot(aes(y=.cooksd,x=.index)) + geom_col() 
    ```
    
    Those spikes in the above plot suggests potential outliers. However, one need a cut-off point to distinguish the outliers from those tamed observations. Unfortunately, there is no a golden rule to split these two groups of points for arbitrary data sets. Some people uses 0.5 or 1 as a cut-off point but our Cook's distances here are much smaller. We will see a refined diagnostic plot for outlier detection in the next step. 
    
6. **We will skip independence in Lab 3 but revisit it in Lab 4**. Before I conclude this exercise, I would like to give you a handy shortcut to produce all above plots. One can easily generate a set of residuals diagnostic plots via `plot()`, i.e the default plot function in R, as follows.

    ```{r}
    plot(lm.youtube)
    ```
    
    The last one of above plots is called **the residuals versus leverage plot**. The **leverage** is another measure for identifying outliers and you shall notice that there are several points being flagged already. This plot will also contain a cut-off curve in the red dash line for Cook's distances if there are any points with Cook's $D>0.5$. 

    Besides of the convenience, another advantage of `plot()` is that the indices of potential outliers will be flagged in the plot. You can use the information in these diagnostic plots to locate the untamed outliers in your data. 
    
    However, one shall notice that, we can't customise those plots by following the same procedures in `ggplot()`. 
    
    *In fact, `plot()` can produce six different diagnostic plots. You can make the individual plot by specifying `which` in `plot()` like `plot(lm.youtube,which=1)` or `plot(lm.youtube,which=4)`. Have a try with `which=1,2,3,4,5,6`  and see which number gives you the desired plot.*


## Exercise 3: Transforming for a better model


We have to acknowledge that the residuals plots of `lm.youtube` suggest that the linear model is not a great one for describing the relationship between `sales` and `youtube`. A transformation is one way to deal with the non-linearity and unequal variance of the data. We will try the log transformation in this exercise. 


1. Instead of modelling `sales` in terms of `youtube`, we could instead take log transforms of `sales` and `youtube` to see if it is possible to get rid of the curvature in the relationship.


    ```{r}
    lm.youtube.log <- lm(log(sales) ~ log(youtube), data=marketing)
    summary(lm.youtube.log)
    ```
    
    
    *Compare the summary output of the two models you have. Which do you think is better? Why?*

2. We add the following code to produce the residual vs fitted plot for this new model. 

    ```{r}
    lm.youtube.log.fit <- augment(lm.youtube.log)
    lm.youtube.log.fit |> ggplot(aes(x=.fitted,y=.std.resid)) +
      geom_point()+geom_smooth()+geom_hline(yintercept=0)
    ```
    
    **What the hell is this curve smoothed by `loess` - the weirdness of it going through the outlier in the bottom left?**
    
    The $y$-axis is now the standardised residuals. If one fit a linear model with a transformation on $y$, the residuals will not be reported and the standardised residuals will be used thoroughly. *Why do we not use the residuals?* 
    
    *How is the assumption of linearity now? What about equal variance? Remember to add comments to your notebook.*

    *Produce other diagnostic plots and add some comments on them.*
  

4. Earlier we produced confidence and prediction intervals for sales with `youtube` budget equal to zero dollar. *Re-do these using your transformed model. Remember that you'll need to exponentiate the resulting intervals using the `exp()` function (Why?)*

    ```{r}

    ```

    *How do these intervals compare with your previous ones? Add some comments to your notebook about this.*

5. Now let's try visualising the second linear model. You should notice that the $x$-axis is on the normal scale (even though we applied a log transformation in the model formula) but the $y$-axis is on the log scale. 

    ```{r}
    library(visreg)
    visreg(lm.youtube.log, gg=TRUE)
    ```

    Nonetheless, the model fit should be a bit better, and will be curved. Notice that we've used a **linear** model to fit a curved relationship. The key is that the linearity of the model is in terms of the coefficients (each term can contain only one $\beta$ as a multiplier, and terms must be added together) not in terms of the way $y$ and $x$ are related. You can apply any transformation you like to $x$ and $y$ as needed to fit the data.

6. You can also visualise the second linear model on the natural scale by applying a transformation in the `visreg` command. Try the following:

    ```{r}
    visreg(lm.youtube.log, trans=exp, partial=TRUE, gg =TRUE)
    ```

    The `trans=exp` uses exponentiation to transform the outcome variable. The `partial=TRUE` means that residuals deviated from the line (and thus data values) are plotted as points as well. You may want to change the y-axis label by adding `ylab="Body weight (kg)"` to the above command.

    *Add some comments to your notebook about the model fit and how well you think it does. Notice that the confidence bands at the right end are larger than those at the left end or in the middle. Why is this?*

7. **Optional Challenge: Try to find a better transform for this data set!** 
