---
title: "Lecture C06"
subtitle: "Multiple factors and interactions"
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "part_c.css"]
    # mathjax: "https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(ggplot2); theme_set(theme_bw(base_size=15))
library(patchwork)
library(fontawesome)
library(visreg)
opts_chunk$set(#dev.args=list(bg='transparent'), 
               comment="", warning=FALSE, echo=FALSE)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(8,5), out.width="70%", fig.retina=2)
```

## Recap

- Multiple predictors

- $F$-test and ANOVA

- Factors

---

## Learning Outcomes

- Interactions between factors

- Interaction between numerical variables and factors

---


class: middle, inverse

## Interactions

---

## Interactions

--

Our current models have assumed that the effects are *additive*. That is, we can get a good estimate of the mean by adding the various effects in our model together.

--

For example, the effects of sex and area on wing lengths of petrels can be added together. The effect of being male (rather than female) is the same across areas, and the effect of being in a particular area is the same across males and females.

--

This is often not the case. Sometimes, the effects of one factor depend on another. For this, we need an *interaction*. 

---

## Example: Hypothetical drugs

--

Suppose we can measure the severity of some medical illness as a response variable. 

--

There are two drugs (A and B) that can be used to treat the condition.

--

The drugs can be administered alone or in combination, giving four possible treatments:
- No drugs
- Just drug A
- Just drug B 
- Both drugs 

--

With the linear models we've used so far, for the "both drugs" case, we would have to assume that we can add the effect of drug A to the effect of drug B to get a good estimate of severity of the illness.

---

## Example: Hypothetical drugs

The model equation for estimating the mean severity of illness $y$ with the different drug treatments would be:

$$
\mathsf{mean}(y) = \alpha + \beta_A z_A + \beta_B z_B
$$
where $z_A$ and $z_B$ are indicators for whether drug A or B are included, giving

| Treatment     | $z_A$ | $z_B$ | Mean  | 
| ------------  |------ |------ |-------| 
| No drugs      |  0    |  0    | $\mathsf{mean}(y)=\alpha$ | 
| Just drug A   |  1    |  0    | $\mathsf{mean}(y)=\alpha + \beta_A$ | 
| Just drug B   |  0    |  1    | $\mathsf{mean}(y)=\alpha + \beta_B$ | 
| Both drugs    |  1    |  1    | $\mathsf{mean}(y)=\alpha + \beta_A + \beta_B$ | 


---

## Example: Hypothetical drugs

--

For the combined drugs treatment, we have $\mathsf{mean}(y)=\alpha + \beta_A + \beta_B$

--

But what if the effect of combining the drugs is less than or greater than the sum of their effects in isolation? Perhaps they cancel each other out. Perhaps the effects are somehow enhanced in combination.

--

If so, we need more flexibility in the model.

---

## Interactions: Hypothetical drugs

We add another indicator variable $z_{AB}$ which is 1 when both drugs are in use. The model is now:
$$\mathsf{mean}(y) = \alpha + \beta_A z_A + \beta_B z_B + \beta_{AB} z_{AB}$$

--

The four treatments are as follows:

| Treatment     | $z_A$ | $z_B$ | $z_{AB}$ | Mean  | 
| ------------  |------ |------ |----------|-------| 
| No drugs      |  0    |  0    |  0       | $\mathsf{mean}(y)=\alpha$ | 
| Just drug A   |  1    |  0    |  0       | $\mathsf{mean}(y)=\alpha + \beta_A$ | 
| Just drug B   |  0    |  1    |  0       | $\mathsf{mean}(y)=\alpha + \beta_B$ | 
| Both drugs    |  1    |  1    |  1       | $\mathsf{mean}(y)=\alpha + \beta_A + \beta_B + \beta_{AB}$ | 

--

The new variable codes for the **interaction effect** of the two drugs together - the combined effect of the drugs *over and above* their additive effects.

--

It is actually just the product of the other two variables: $z_{AB}= z_A \times z_B$

Hence, interactions are sometimes called 'multiplicative effects'. 

---

## Interactions: Hypothetical drugs

--

The mean severity of illness $y$ for patients that received both drugs is:
$$\mathsf{mean}(y) = \alpha + \beta_A + \beta_B + \beta_{AB}$$
--

If the interaction parameter $\beta_{AB}$ is zero, then the effects of the drugs are simply additive and no interaction is required. 

--

If $\beta_{AB}$ is non-zero, then there is some interactive effect of taking both drugs that is not accounted for by adding their effects together.

--

Therefore, testing the null hypothesis that $\beta_{AB} = 0$ is a test of whether or not the two drugs are additive or interactive. 

---

## Example: Hypothetical drugs

<iframe src="https://shiny.massey.ac.nz/jcmarsha/twoway/" height="300" width="800" style="border: none"></iframe>

https://shiny.massey.ac.nz/jcmarsha/twoway/

---

## Interactions in R: petrel data

```{r, echo=TRUE}
petrels <- read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/petrels.csv") %>% 
  transmute(`Wing length`=R.Wing.Lth, Area=as_factor(Area), Sex=as_factor(Sex)) %>% drop_na()

# model without interaction
mod_no_int = lm(`Wing length` ~ Sex + Area, petrels)

# model with interaction
mod_int = lm(`Wing length` ~ Sex + Area + Sex:Area, petrels)
```


---

## Interactions in R: petrel data

.left-code-wide[
.small[

```{r}

anova(mod_int)
```
]]

```{r, echo=FALSE}
pval=round(anova(mod_int)[,5],3)
names(pval)=rownames(anova(mod_int))
```

--

.right-code[

The interaction term is given by `Sex:Area`.

The `anova` table shows the interaction term is not significant (P=`r pval["Sex:Area"]`).

Thus, there is no evidence in the data to suggest that the difference in wing length between sexes differs among areas. 

The 'main effects' (i.e., not interactions) of Sex and Area are both highly significant.

]


---


.left-code-wide[
.small[
### Interpreting coefficients with interactions
```{r, echo=FALSE}
summary.lm(mod_int)$coef %>% round(.,2)
co3=round(coef(mod_int),2)
```

]]

---


.left-code-wide[
.small[
### Interpreting coefficients with interactions
```{r, echo=FALSE}
summary.lm(mod_int)$coef %>% round(.,2)
co3=round(coef(mod_int),2)
```

]]


.right-code[
.small[


The interaction term isn't significant but we'll interpret the estimates anyway.


]]

---


.left-code-wide[
.small[
### Interpreting coefficients with interactions
```{r, echo=FALSE, highlight.output=2}
summary.lm(mod_int)$coef %>% round(.,2)
co3=round(coef(mod_int),2)
```

]]


.right-code[
.small[

The interaction term isn't significant but we'll interpret the estimates anyway.

The `Intercept` represents the estimated mean for baseline levels of the factors. So, wings of females in Area 1 were `r co3["(Intercept)"]` mm long, on average.


]]

---


.left-code-wide[
.small[
### Interpreting coefficients with interactions
```{r, echo=FALSE, highlight.output=3}
summary.lm(mod_int)$coef %>% round(.,2)
co3=round(coef(mod_int),2)
```

]]


.right-code[
.small[


The interaction term isn't significant but we'll interpret the estimates anyway.

The `Intercept` represents the estimated mean for baseline levels of the factors. So, wings of females in Area 1 were `r co3["(Intercept)"]` mm long, on average.

The `SexMale` coefficient is the estimated difference between males and females, keeping the other variable (Area) constant. In Area 1, males' wings were `r co3["SexMale"]` mm longer than females', on average.


]]

---


.left-code-wide[
.small[
### Interpreting coefficients with interactions
```{r, echo=FALSE, highlight.output=4}
summary.lm(mod_int)$coef %>% round(.,2)
co3=round(coef(mod_int),2)
```

]]


.right-code[
.small[


The interaction term isn't significant but we'll interpret the estimates anyway.

The `Intercept` represents the estimated mean for baseline levels of the factors. So, wings of females in Area 1 were `r co3["(Intercept)"]` mm long, on average.

The `SexMale` coefficient is the estimated difference between males and females, keeping the other variable (Area) constant. In Area 1, males' wings were `r co3["SexMale"]` mm longer than females', on average.

The `Area2` coefficient is the estimated difference between Area 2 and Area 1, keeping the other variable (Sex) constant. Wings of females in Area 2 were `r -co3["Area2"]` mm smaller than those in Area 1, on average.


]]

---


.left-code-wide[
.small[
### Interpreting coefficients with interactions
```{r, echo=FALSE, highlight.output=9}
summary.lm(mod_int)$coef %>% round(.,2)
co3=round(coef(mod_int),2)
```

]]


.right-code[
.small[


The interaction term isn't significant but we'll interpret the estimates anyway.

The `Intercept` represents the estimated mean for baseline levels of the factors. So, wings of females in Area 1 were `r co3["(Intercept)"]` mm long, on average.

The `SexMale` coefficient is the estimated difference between males and females, keeping the other variable (Area) constant. In Area 1, males' wings were `r co3["SexMale"]` mm longer than females', on average.

The `Area2` coefficient is the estimated difference between Area 2 and Area 1, keeping the other variable (Sex) constant. Wings of females in Area 2 were `r -co3["Area2"]` mm smaller than those in Area 1, on average.

The `SexMale:Area2` is the extra effect of **the combination of Male & Area 2**, over and above the marginal effects of Male and Area 2. 

]]


---


.left-code-wide[
.small[
### Interpreting coefficients with interactions
```{r, echo=FALSE, highlight.output=c(3,4,9)}
summary.lm(mod_int)$coef %>% round(.,2)
co3=round(coef(mod_int),2)
```

]]


.right-code[
.small[


The interaction term isn't significant but we'll interpret the estimates anyway.

The `Intercept` represents the estimated mean for baseline levels of the factors. So, wings of females in Area 1 were `r co3["(Intercept)"]` mm long, on average.

The `SexMale` coefficient is the estimated difference between males and females, keeping the other variable (Area) constant. In Area 1, males' wings were `r co3["SexMale"]` mm longer than females', on average.

The `Area2` coefficient is the estimated difference between Area 2 and Area 1, keeping the other variable (Sex) constant. Wings of females in Area 2 were `r -co3["Area2"]` mm smaller than those in Area 1, on average.

The `SexMale:Area2` is the extra effect of **the combination of Male & Area 2**, over and above the marginal effects of Male and Area 2. 

Compared to females in Area 1, wings of males in Area 2 were $`r co3["SexMale"]`+`r co3["Area2"]`+`r co3["SexMale:Area2"]`=`r sum(co3[c("SexMale","Area2","SexMale:Area2")])`$ mm larger, on average.

]]
---

## Visualising models using `visreg`

Plotting the estimated means is very useful for interpreting a model. 

When you have interactions, you must interpret the effects of one variable within the context of another.

```{r, echo=TRUE, eval=T, fig.height=4, fig.show='hold', out.width="50%"}
library(visreg)
visreg(mod_int, "Area", by="Sex", gg=TRUE)
visreg(mod_int, "Area", by="Sex", gg=TRUE, overlay=TRUE)
```

---

.pull-left[
.small[
.center[
### Model without interaction
```{r, fig.width=8, fig.height=4, out.width="170%"}
visreg(mod_no_int, "Area", by="Sex", gg=TRUE)
```
]
- Pattern across areas is **the same** for the two sexes.

- **Less uncertainty** in estimates of means. 

]]

.pull-right[
.small[
.center[
### Model with interaction
```{r, fig.width=8, fig.height=4, out.width="170%"}
visreg(mod_int, "Area", by="Sex", gg=TRUE)
```
]
- Pattern across areas is **different** for the two sexes.

- **More uncertainty** in estimates of means (especially those with low sample sizes). 

]]




---

.pull-left[
.small[
.center[
### Model without interaction
```{r, fig.width=8, fig.height=4, out.width="170%"}
visreg(mod_no_int, "Sex", by="Area", gg=TRUE)
```
]
- Differences between sexes is **the same** across Areas.

- **Less uncertainty** in estimates of means. 

]]

.pull-right[
.small[
.center[
### Model with interaction
```{r, fig.width=8, fig.height=4, out.width="170%"}
visreg(mod_int, "Sex", by="Area", gg=TRUE)
```
]
- Differences between sexes is **different** across Areas.

- **More uncertainty** in estimates of means (especially those with low sample sizes). 

]]



---

.pull-left[
.small[
.center[
### Model without interaction
```{r, fig.width=8, fig.height=4, out.width="170%"}
visreg(mod_no_int, "Area", by="Sex", 
       overlay= TRUE, gg=TRUE)
```
]
- Differences between sexes is **the same** across Areas.

- **Less uncertainty** in estimates of means. 

]]

.pull-right[
.small[
.center[
### Model with interaction
```{r, fig.width=8, fig.height=4, out.width="170%"}
visreg(mod_int, "Area", by="Sex", 
       overlay= TRUE, gg=TRUE)
```
]
- Differences between sexes is **different** across Areas.

- **More uncertainty** in estimates of means (especially those with low sample sizes). 

]]

---

class: middle, inverse

# Other types of interaction: numeric variables and/or factors

---

## Model with a numeric variable and a factor (no interaction)

--

Say we had one numeric variable $x$ and one indicator variable for the 2nd level of a factor $z$.

--

We can combine them in a linear model:

$$
\mathsf{mean}(y) = \alpha + \beta_1 x + \beta_2 z
$$
--

For the first level of the factor, $z=0$ so the equation is
$$
\mathsf{mean}(y) = \alpha + \beta_1 x
$$
--

For the second level of the factor, $z=1$ so the equation is
$$
\begin{aligned}
\mathsf{mean}(y) &= \alpha + \beta_1 x + \beta_2\\
 &= (\alpha + \beta_2) + \beta_1 x
\end{aligned}
$$

--

The model fits two parallel lines, one for each level of the factor. Their slopes are the same but they have different intercepts: $\alpha$ for level 1, and $\alpha + \beta_2$ for level 2. 


---

## Example: Calf weights

```{r, echo=TRUE, fig.dim=c(7,4), fig.height = "50%", message=FALSE}
calf <- read_csv("https://www.massey.ac.nz/~jcmarsha/227215/data/calfweight.csv")
ggplot(calf, aes(x=Age, y=Weight, col=Breed, shape=Treatment)) + 
  geom_point(size=3, alpha=0.6) + xlab("Age (days)") + ylab("Weight (kg)")
```


---

## Example: Calf weights

```{r}
c1_no_int = lm(Weight ~ BirthWeight + Breed + Treatment + Age, data=calf)
anova(c1_no_int)
```

---

## Example: Calf weights

```{r, fig.width=8, fig.height=5, fig.align='center'}
visreg(c1_no_int, "Age", by="Treatment", overlay=TRUE, gg=TRUE)
```

---

## Example: Calf weights

```{r, fig.width=8, fig.height=5, fig.align='center'}
visreg(c1_no_int, "Age", by="Breed", overlay=TRUE, gg=TRUE)
```

---

## Interaction of numeric variables and factors

--

So far, we've built models that combine a numeric variable $x$ and an indicator for level 2 of a fector $Z$: 

$$
\mathsf{mean}(y) = \alpha + \beta_1 x + \beta_2 z
$$
--

Now, let's include an interaction term by fitting a coefficient, $\beta_3$, to the product of $x$ and $z$: 
$$
\mathsf{mean}(y) = \alpha + \beta_1 x + \beta_2 z + \beta_3 x z
$$

--

For the first level of the factor, $z=0$ so the equation is:
$$
\mathsf{mean}(y) = \alpha + \beta_1 x
$$

--

For the second level, $z=1$ so the equation is:
$$
\begin{align}
\mathsf{mean}(y) &= \alpha + \beta_1 x + \beta_2 + \beta_3x\\
 &= (\alpha + \beta_2) + (\beta_1 + \beta_3) x
\end{align}
$$

--

Thus, the model fits two different lines for each level of the factor, with different intercepts and different slopes.

---

## Example: Calf weights

```{r, echo=FALSE, fig.width=8, fig.height=5.5, fig.align='center'}
ggplot(calf, aes(x=Age, y=Weight, col=Breed, shape=Treatment)) + 
  geom_point(size=3, alpha=0.8) + xlab("Age (days)") + ylab("Weight (kg)")
```

---

## Calf weights: anova summary

```{r}
c2_int <- lm(Weight ~ BirthWeight + Breed + Treatment + Age +
           Age:Treatment + Age:Breed, data=calf)
anova(c2_int)
```

---

## Calf weights: linear model summary
.small[
```{r, echo=FALSE}
summary(c2_int)
```
]

---

.pull-left[
.small[
.center[
### Without interactions
```{r, fig.width=8, fig.height=5, out.width = "100%", fig.align='center'}
visreg(c1_no_int, "Age", by="Breed", 
       overlay=TRUE, gg=TRUE)
```
]
- Same slopes

]]

.pull-right[
.small[
.center[
### With interactions
```{r, fig.width=8, fig.height=5, out.width = "100%", fig.align='center'}
visreg(c2_int, "Age", by="Breed", 
       overlay=TRUE, gg=TRUE)
```
]
- Different slopes
]]



---

.pull-left[
.small[
.center[
### Without interactions
```{r, fig.width=8, fig.height=5, out.width = "100%", fig.align='center'}
visreg(c1_no_int, "Age", by="Treatment", 
       overlay=TRUE, gg=TRUE)
```
]
- Same slopes

]]

.pull-right[
.small[
.center[
### With interactions
```{r, fig.width=8, fig.height=5, out.width = "100%", fig.align='center'}
visreg(c2_int, "Age", by="Treatment", 
       overlay=TRUE, gg=TRUE)
```
]
- Different slopes
]]


---

## Always check the diagnostic plots!

.left-code-wide[
.small[
```{r, echo=TRUE, eval=F}
plot(c2_int)
```

```{r, echo=FALSE, fig.width=7.5, fig.height=5, out.width = "100%"}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(c2_int)
```

]]

--

.right-code[

This is bad!

The left two plots show that the variance of the residuals increases with the fitted values. 

We are violating an important assumption of the linear model, and so we cannot trust any conclusions we might have made from it. 

]


---

## Log calf weights

```{r}
c3_int_log <- lm(log(Weight) ~ log(BirthWeight) + Breed + Treatment + Age + 
         Age:Treatment + Age:Breed, data=calf)
anova(c3_int_log)
```

--

The interactions are still significant. 

This is not always the case. Sometimes, log-transformations remove the need for interactions.


---

## Log calf weights: Diagnostics

.left-code-wide[
.small[
```{r, echo=TRUE, eval=F}
plot(c3_int_log)
```

```{r, echo=FALSE, fig.width=7.5, fig.height=5, out.width = "100%"}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(c3_int_log)
```

]]

.right-code[

Better!

]

---

## Visualising the fit

```{r, fig.width=8, fig.height=4, out.width="50%"}
visreg(c3_int_log, "Age", by="Breed", trans=exp, 
       partial=TRUE, overlay=TRUE, ylab="Weight", gg=TRUE)
```

---

## Visualising the fit

```{r, fig.width=8, fig.height=4, out.width="50%"}
visreg(c3_int_log, "Age", by="Treatment", trans=exp, 
       partial=TRUE, overlay=TRUE, ylab="Weight", gg=TRUE)
```

--

We should be careful with this model though. It implies that calves grow exponentially, which probably cannot be sustained for very long! Otherwise, after a short time, the calves would be bigger than a house.

---

## Summary

- Interactions are where the effect of one predictor variable on the response variable *depends on the value of another predictor variable*

- Interaction between two binary factors

- Interaction between two general factors

- Interaction between a numerical variable and a factor

- Not covered: interaction between two numerical variables

