---
title: "Lecture C05"
subtitle: "Linear models with multiple predictors and factors"
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "part_c.css"]
    # mathjax: "https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---


```{r setup, echo=FALSE}
library(knitr)
library(ggplot2); theme_set(theme_bw(base_size=15))
library(patchwork)
library(fontawesome)
opts_chunk$set(#dev.args=list(bg='transparent'), 
               comment="", warning=FALSE, echo=FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(8,5), out.width="70%", fig.retina=2)
```

```{r, echo=FALSE}
donkey <- read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
```

## Recap

- Simple linear model

- Inference

- Prediction 

- Theory

- Diagnostics

- Transformation

---

## Learning Outcomes

- Adding more predictor variables.

- The $F$-test.

- Models with factor variables.

- Multiple factors

---

class: middle, inverse

## The linear model with multiple predictors

---

## Relationships: Donkeys

```{r, fig.align="left", echo=TRUE, fig.dim = c(14,3), out.width="100%"}
donkey <- read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
g1 <- ggplot(donkey, aes(x=Heartgirth, y=Bodywt)) + geom_point(col="#7f577492")
g2 <- ggplot(donkey, aes(x=Umbgirth, y=Bodywt)) + geom_point(col="#7f577492")
g3 <- ggplot(donkey, aes(x=Length, y=Bodywt)) + geom_point(col="#7f577492")
g4 <- ggplot(donkey, aes(x=Height, y=Bodywt)) + geom_point(col="#7f577492")
g5 <- ggplot(donkey, aes(x=Sex, y=Bodywt)) + geom_boxplot(col="#7f577492") 
g1 + g2 + g3 + g4 + g5 + plot_layout(nrow=1) 
```

---

## Relationships: Donkeys

--

There are reasonably strong increasing relationships between *body weight* and the continuous variables (*hearth girth*, *umbilical girth*, *length*, and *height*). There is no apparent relationship with *sex.*

--

We could fit simple regression models of *body weight* with each of the continuous variables in turn.

.center[
```
Bodywt ~ Heartgirth
Bodywt ~ Umbgirth
Bodywt ~ Length
Bodywt ~ Height
```
]

--

Alternatively, we can fit one multiple regression model of *body weight* that uses *all four variables simultaneously*.

.center[
```
Bodywt ~ Heartgirth + Umbgirth + Length + Height
```
]

--

What are the potential advantages of this? 

--

A model that uses all variables *may* lead to more better and more precise estimates, with narrower confidence and prediction intervals. 

--

However, we must be careful because a more complex model might make worse predictions than a simpler model, and it's not always obvious when this is the case!

---

## Multiple predictors

A multiple regression model is simply the sum of effects of the predictor variables:
$$
\mathsf{mean}(y) = \alpha + \beta_1 x_1 + \beta_2 x_2 + \cdots
$$

--

We can use the least squares criterion to estimate $\alpha$, $\beta_1, \beta_2, \ldots$, just as we did for simple linear regression. We find the values of the parameters that minimise the sum of squared residuals.

--

As the model assumptions are still defined in terms of residuals, the mathematics and practicality of fitting the model doesn't change a lot.

--

However, we get a bigger table of coefficients, and there are some important concepts to be aware of when interpreting the results.

---

## Sidenote: Mathematical form of the linear model

With $p$ predictors $x_1,x_2,\ldots,x_p$ for a single response $y$, the abstract sample becomes a little more complicated as
$(y_i,x_{i1},x_{i2},\ldots,x_{ip})$ with $i=1,2,\ldots,n$.

We can write the model equation for each individual observation as
$$
\begin{aligned}
y_1 &=\alpha+ \beta_1x_{11}+\beta_2 x_{12} +\cdots\beta_px_{1p} +\epsilon_1\\
y_2 &=\alpha+ \beta_1x_{21}+\beta_2 x_{22} +\cdots\beta_px_{2p} +\epsilon_2\\
&\vdots\\
y_n &=\alpha+ \beta_1x_{n1}+\beta_2 x_{n2} +\cdots\beta_px_{np} +\epsilon_n
\end{aligned}
$$

---

## Regression models with one *vs* two predictors

.pull-left[
.small[

#### A simple regression model   


```{r echo=TRUE }
lm(Bodywt ~ Heartgirth, donkey) %>% 
  summary %>% coef %>% round(.,2)
```
<br>

Plug in the coefficients:    

$\mathsf{Mean(Bodywt)} =-194.49 + \color{red}{2.83} \mathsf{Heartgirth}$
]]

--

.pull-right[
.small[

#### A multiple regression model   

```{r echo=TRUE}
lm(Bodywt ~ Heartgirth + Umbgirth, donkey) %>% 
  summary %>% coef %>% round(.,2)
```  

<br>

Plug in the coefficients:

$$\mathsf{Mean(Bodywt)} =-194.49 + \color{red}{2.39}\mathsf{Heartgirth} + 0.42 \mathsf{Umbgirth}$$
]]

--
.center[
<br>

### Why has the coefficient for `Heartgirth` changed?!

]

---

## Testing multiple predictors

--

Our first question might be:    
> ***Are any predictor variables linearly related to the response variable?***

--

In the case of our donkeys, they four of the predictors are clearly related to *body weight*. Fitting a simple linear regression to each one in turn, they'd all be significant.

--

When considering multiple regression, we ask a slightly different question:     
> ***Which predictor variables are linearly related to the response variable*** **when accounting for all the predictor variables simultaneously?**

--

In other words:  
> **Which of the $\beta$ slopes are non-zero when they're all in the model together?**

--

An important fact:     
If you take a model $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathsf{y} = \alpha + \beta_1 x_1$     
and add another predictor $\ \ \ \ \ \ \ \ \ \ \ \ \mathsf{y} = \alpha + \beta_1 x_1 + \beta_2 x_2$    
**our estimates of $\alpha$ and $\beta_1$ will change!** 

--

Why? The predictor variables are nearly always, to some extent, correlated with each other, so they *compete with each other* to explain the same variation in the response. Each piece of the pie can only be eaten once!

---

## Multiple linear regression

.left-code-wide[

.small[

```{r echo=FALSE}
mod <- lm(Bodywt ~ Heartgirth + Umbgirth + Length + Height, data=donkey)
summary(mod)
```

]
]

---

## Multiple linear regression

.left-code-wide[

.small[

```{r echo=FALSE, highlight.output=13:15}
mod <- lm(Bodywt ~ Heartgirth + Umbgirth + Length + Height, data=donkey)
summary(mod)
```

]
]

--

.right-out-narrow[

There is very strong evidence for effects of *heart girth*, *umbilical girth*, and *length*, when accounting for other predictors.

]


---

## Multiple linear regression

.left-code-wide[

.small[

```{r echo=FALSE, highlight.output=16}
mod <- lm(Bodywt ~ Heartgirth + Umbgirth + Length + Height, data=donkey)
summary(mod)
```

]
]

.right-out-narrow[

There is very strong evidence for effects of *heart girth*, *umbilical girth*, and *length*, when accounting for other predictors.

There is only weak evidence for an effect of *height* when accounting for the other predictors.

]

---

## Multiple linear regression

.left-code-wide[

.small[

```{r echo=FALSE, highlight.output=16}
mod <- lm(Bodywt ~ Heartgirth + Umbgirth + Length + Height, data=donkey)
summary(mod)
```

]
]

.right-out-narrow[

There is very strong evidence for effects of *heart girth*, *umbilical girth*, and *length*, when accounting for other predictors.

There is only weak evidence for an effect of *height* when accounting for the other predictors.

**This doesn't mean that** ***height*** **isn't related to body weight!**

]

---

## (Not) multiple linear regression

.left-code-wide[

.small[

```{r echo=FALSE, highlight.output=c(3,12)}
summary(lm(Bodywt ~ Height, data=donkey))
```

]
]

--

.right-out-narrow[

A model with *only height* shows plenty of evidence that *height* is related to *body weight*. However, no such evidence is apparent from the model with all four predictors. **Why?**

]

---

## (Not) multiple linear regression

.left-code-wide[

.small[

```{r echo=FALSE, highlight.output=c(3,12)}
summary(lm(Bodywt ~ Height, data=donkey))
```

]
]

.right-out-narrow[

A model with *only height* shows plenty of evidence that *height* is related to *body weight*. However, no such evidence is apparent from the model with all four predictors. **Why?**

*Height* is correlated with the other predictors (*heart girth*, *umbilical girth*, and *length*). 

When we include the other predictors in the model, *height* cannot explain much variation in *body weight* that isn't already explained by the other predictors. 

]


---

## (Not) multiple linear regression

.left-code-wide[
.small[

```{r echo=FALSE, highlight.output=c(3,12)}
summary(lm(Bodywt ~ Height, data=donkey))
```

]
]

.right-out-narrow[

A model with *only height* shows plenty of evidence that *height* is related to *body weight*. However, no such evidence is apparent from the model with all four predictors. **Why?**

*Height* is correlated with the other predictors (*heart girth*, *umbilical girth*, and *length*). 

When we include the other predictors in the model, *height* cannot explain much variation in *body weight* that isn't already explained by the other predictors. 

Once you know *heart girth*, *umbilical girth*, and *length*, knowing *height* isn't that useful.

**In multiple regression, each predictor is assessed on the variation in the response that** ***it and it alone*** **can explain**.

]

---

## Multiple linear regression

.left-code-wide[
.small[

```{r echo=FALSE, highlight.output=21}
summary(lm(Bodywt ~ Heartgirth + Umbgirth + Length + Height, data=donkey))
```

]
]

```{r echo=FALSE}
rs <- round(summary(mod)$r.squared,2)
fv <- round(summary(mod)$fstatistic[1],1)
```


.right-out-narrow[

The $R^2$ for the model is `r rs`. This means the model explains `r rs*100`% of the variation in *body weight*.

Compare this to the model with *height* alone, which explained 61% of the variation in *body weight*. Three more variables takes us from 61% to `r rs*100`%. This implies some *redundancy* in the explanatory power of the variables.

]

---


## Multiple linear regression

.left-code-wide[

.small[

```{r echo=FALSE, highlight.output=22}
summary(lm(Bodywt ~ Heartgirth + Umbgirth + Length + Height, data=donkey))
```

]
]

.right-out-narrow[

The $R^2$ for the model is `r rs`. This means the model explains `r rs*100`% of the variation in *body weight*.

Compare this to the model with *height* alone, which explained 61% of the variation in *body weight*. Three more variables takes us from 61% to `r rs*100`%. This implies some *redundancy* in the explanatory power of the variables.

Next question:    
***How do we interpret the `F-statistic` and `p-value` of a multiple regression model?***

]

---

class: middle, inverse

## The F-test

---

## The F-test

Some useful general questions for multiple regression:   
> *Is our model is useful at all?*    
*Does our model explain any variation in the response variable?*

--

If not, our null hypothesis is:   
> *None of the predictor variables explain variation in the response variable.*

--

Or, equivalently:
> *All the slope coefficients ( $\beta$s ) are zero.*

--

Thus, the F-test is an 'omnibus' test of the whole model collectively, as opposed to the t-tests in the coefficients table, which tests the contribution of each predictor variable. 

---

## The F-test

$$F = \frac{ \mathsf{ Variation\ explained }}{\mathsf{Variation\ unexplained}} = \frac{(\sigma^2_\mathsf{total} - \sigma^2_\mathsf{res})/p} {\sigma^2_\mathsf{res}/(n-p-1)}$$

where $p$ is the number of predictors, and $n$ the number of observations.

--

The numerator of the F-statistic relates to the variance explained by the model. The denominator relates to the variance in residuals, the unexplained variance. 

--

F-statistic is a ratio - it gets larger the more variation in $y$ the model is able to explain. 

--

Under the null hypothesis (none of the predictors truly explain any variance in $y$), the explained and unexplained variation is roughly equal and $F$ should be around 1. ***Why?***

--

Even when none of *none* of the predictors are truly related to $y$ in the population, just by chance, the model will *always* explain some variation. 

--

If the predictors are indeed related to $y$, we should get $F > 1$.

--

The associated hypothesis test is known as **ANalysis Of VAriance** or **ANOVA**. The P-value measures the degree of evidence against the null hypothesis. The greater the evidence in the data, the higher the F, and the lower the P.

---

## The F-statistic in summary output

```
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -222.33956    8.01411 -27.744  < 2e-16 ***
Heartgirth     1.76223    0.12784  13.784  < 2e-16 ***
Umbgirth       0.37418    0.06726   5.563 5.01e-08 ***
Length         0.87580    0.11531   7.595 2.41e-13 ***
Height         0.25407    0.13749   1.848   0.0654 .  
---

Residual standard error: 9.476 on 380 degrees of freedom
Multiple R-squared:  0.8519,	Adjusted R-squared:  0.8503 
F-statistic: 546.4 on 4 and 380 DF,  p-value: < 2.2e-16
```

--

The tiny `p-value` tells us that the large F-statistic of `r fv` is very unlikely to have arisen just by chance if the null hypothesis is true.

--

Thus, we reject the null hypothesis and conclude that we have very strong evidence that our linear model explains variation *body weight*, and that at least one slope is not zero.

---

class: middle, inverse

## Linear models with factors

---

## Linear models with factors

--

The linear model equation estimates the **mean** of the response variable given the value of the predictors.

--

We have so far dealt with *numerical* predictors; for example, $x = [34.6,45.1, 40.2, 37.9, ...]$

--

Sometimes, the predictor variables are *categorical*; for example, $x = [A,A,A,A,B,B,B,B,C,C,C,C, ...]$

--

Categorical vectors are often called 'factors'. Factors have two or more 'levels'.    
'A', 'B', and 'C' are the levels of the factor $x$ above.

--

Each level delineates a group of $y$ values.     
**A linear model fit to a factor can give estimates of the mean of $y$ in each group.**


--

To fit such a model, we need to convert the factor into numbers. There are many ways of doing this.    
(It is best to avoid labelling the levels with numbers, "1", "2", "3", etc., because you might mistakenly treat the variable as continuous.)

---

## Factors with two levels

--

Consider a factor with two levels, say, $\mathsf{A}$ and $\mathsf{B}$. We have some $y$ values from group $\mathsf{A}$ and some $y$ values from group $\mathsf{B}$. We want to estimate the mean of $y$ for each group, $\mu_\mathsf{A}$ and $\mu_\mathsf{A}$, and test whether these means are different.

--

We can codify a factor with two levels using an '*indicator*' (or '*dummy*') variable $z$, where
$$z = \left\{
\begin{array}{ll}
0 & \textrm{for observations in level } \mathsf{A}\\
1 & \textrm{for observations in level } \mathsf{B}
\end{array}\right.$$

--

```{r}
tibble(y = rnorm(6), 
       x = c("A","A","A","B","B","B")) %>% 
  mutate(z = ifelse(x=="B",1,0))
```


---

## Factors with two levels



Consider a factor with two levels, say, $\mathsf{A}$ and $\mathsf{B}$. We have some $y$ values from group $\mathsf{A}$ and some $y$ values from group $\mathsf{B}$. We want to estimate the mean of $y$ for each group, $\mu_\mathsf{A}$ and $\mu_\mathsf{A}$, and test whether these means are different.

We can codify a factor with two levels using an '*indicator*' (or '*dummy*') variable $z$, where
$$z = \left\{
\begin{array}{ll}
0 & \textrm{for observations in level } \mathsf{A}\\
1 & \textrm{for observations in level } \mathsf{B}
\end{array}\right.$$

--

$z$ is then used as a predictor in a linear model for the mean $\ \mu = \alpha + \beta z$

--

For level $\mathsf{A}$, $z=0$, and the equation reduces to $\ \mu_\mathsf{A} = \alpha + \beta \times 0 = \alpha$

--

For level $\mathsf{B}$, $z=1$, and the equation reduces to $\ \mu_\mathsf{B} = \alpha + \beta \times  1 = \alpha + \beta$

--

<br>

Thus, $\beta$ represents the **difference** between the mean of $y$ for level $\mathsf{A}$  and the mean of $y$ for level $\mathsf{B}$.

--

To test for a difference in the means of groups `A` and `B`, we use the null hypothesis:   
$\mathsf{H_0}: \mu_\mathsf{A} = \mu_\mathsf{B}$ $\textrm{} \ \ \ \ \ \ \textrm{}$ or, equivalently,   
$\mathsf{H_0}: \beta = 0$.

---


### Example: donkey weight vs sex 

.left-code-wide[

Recall the donkey dataset with its two-level factor, `Sex`.

```{r echo=FALSE, fig.dim=c(4,4)}
g5
```
]

--

.right-code[

Not much difference apparent there! But let's do a formal test - is there any evidence for a difference between the average body weights of male and female donkeys?
]

---

### Example: donkey weight vs sex 

.left-code-wide[

.small[
```{r, echo=FALSE, highlight.output=12}
mod <- lm(Bodywt ~ Sex, data=donkey)
summary(mod)
```

]
]

--

.right-code[
`R` automatically creates the indicator variable for us, when given a factor or 'character' vector as a predictor variable. 

By default, `R` puts the levels in alphabetical order so that, for example,    
$\mu_\mathsf{A} = \alpha$ and $\mu_\mathsf{B} = \alpha + \beta$.

Here, the $\beta$ parameter is called '`SexMale`'. 

This tells us that the indicator variable is    
`0` when `Sex = 'Female'` and    
`1` when `Sex = 'Male'`. 

'Female' comes before 'Male' in the alphabet, accordingly.

]

---

### Example: donkey weight vs sex 

.left-code-wide[

.small[
```{r, echo=FALSE, highlight.output=11}
mod <- lm(Bodywt ~ Sex, data=donkey)
summary(mod)
```

]
]

--

.right-code[

The estimated mean body weight of females is given by the intercept,    
$\hat\mu_{\mathsf{F}}=\hat\alpha=121.4 \ \mathsf{kg}$

]

---

### Example: donkey weight vs sex 

.left-code-wide[

.small[
```{r, echo=FALSE, highlight.output=12}
mod <- lm(Bodywt ~ Sex, data=donkey)
summary(mod)
```

]
]

.right-code[

The estimated mean body weight of females is given by the intercept,    
$\hat\mu_{\mathsf{F}}=\hat\alpha=121.4 \ \mathsf{kg}$

The estimated difference between the mean body weights of males and females is given by the slope,    
$\hat\beta=\hat\mu_{\mathsf{M}}-\hat\mu_{\mathsf{F}}=1.8 \ \mathsf{kg}$

]


---

### Example: donkey weight vs sex 

.left-code-wide[
.small[
```{r, echo=FALSE, highlight.output=11:12}
mod <- lm(Bodywt ~ Sex, data=donkey)
summary(mod)
```

]
]

.right-code[

The estimated mean body weight of females is given by the intercept,    
$\hat\mu_{\mathsf{F}}=\hat\alpha=121.4 \ \mathsf{kg}$

The estimated difference between the mean body weights of males and females is given by the slope,    
$\hat\beta=\hat\mu_{\mathsf{M}}-\hat\mu_{\mathsf{F}}=1.8 \ \mathsf{kg}$

The estimated mean body weight of males is given by the intercept + slope,   
$\hat\mu_{\mathsf{M}}=\hat\alpha+\hat\beta=121.4+1.8=123.2 \ \mathsf{kg}$

]


---

### Example: donkey weight vs sex 

.left-code-wide[
.small[
```{r, echo=FALSE, highlight.output=c(12,18)}
mod <- lm(Bodywt ~ Sex, data=donkey)
summary(mod)
```

]
]

.right-code[

The estimated mean body weight of females is given by the intercept,    
$\hat\mu_{\mathsf{F}}=\hat\alpha=121.4 \ \mathsf{kg}$

The estimated difference between the mean body weights of males and females is given by the slope,    
$\hat\beta=\hat\mu_{\mathsf{M}}-\hat\mu_{\mathsf{F}}=1.8 \ \mathsf{kg}$

The estimated mean body weight of males is given by the intercept + slope,   
$\hat\mu_{\mathsf{M}}=\hat\alpha+\hat\beta=121.4+1.8=123.2 \ \mathsf{kg}$

The high p-value means we failed to reject the null hypothesis ( $\mathsf{H_0}: \mu_{\mathsf{F}}=\mu_{\mathsf{M}}$ & $\beta=0$ ) and conclude that there is no evidence for a difference in the mean body weight of female and male donkeys. 

]

---



### Example: donkey weight vs sex 

.left-code-wide[
```{r fig.dim=c(4,5), echo=FALSE}
library(visreg); visreg(mod, alpha=1)
```
]

.right-code[

The estimated mean body weight of females is given by the intercept,    
$\hat\mu_{\mathsf{F}}=\hat\alpha=121.4 \ \mathsf{kg}$

The estimated difference between the mean body weights of males and females is given by the slope,    
$\hat\beta=\hat\mu_{\mathsf{M}}-\hat\mu_{\mathsf{F}}=1.8 \ \mathsf{kg}$

The estimated mean body weight of males is given by the intercept + slope,   
$\hat\mu_{\mathsf{M}}=\hat\alpha+\hat\beta=121.4+1.8=123.2 \ \mathsf{kg}$

The high p-value means we failed to reject the null hypothesis ( $\mathsf{H_0}: \mu_{\mathsf{F}}=\mu_{\mathsf{M}}$ & $\beta=0$ ) and conclude that there is no evidence for a difference in the mean body weight of female and male donkeys. 

]


---

## What about factors with three levels?

--

For two levels, we introduced an indicator variable $z$ that was `1` for level `B` and `0` otherwise. 

--

For three levels, $\mathsf{A}$, $\mathsf{B}$, and $\mathsf{C}$, we need two indicator variables:


$$\begin{align}
z_2 = \left\{
\begin{array}{ll}
1 & \textrm{for observations in level } \mathsf{B}\\
0 & \textrm{otherwise}
\end{array}\right.
&&
z_3 = \left\{
\begin{array}{ll}
1 & \textrm{for observations in level } \mathsf{C}\\
0 & \textrm{otherwise}
\end{array}\right.
\end{align}$$


--
.smaller[
```{r}
tibble(y = rnorm(9), 
       x = c("A","A","A","B","B","B","C","C","C")) %>% 
  mutate(z2 = ifelse(x=="B",1,0),
         z3 = ifelse(x=="C",1,0))
```
]

---

## What about factors with three levels?


For two levels, we introduced an indicator variable $z$ that was `1` for level `B` and `0` otherwise. 


For three levels, $\mathsf{A}$, $\mathsf{B}$, and $\mathsf{C}$, we need two indicator variables:


$$\begin{align}
z_2 = \left\{
\begin{array}{ll}
1 & \textrm{for observations in level } \mathsf{B}\\
0 & \textrm{otherwise}
\end{array}\right.
&&
z_3 = \left\{
\begin{array}{ll}
1 & \textrm{for observations in level } \mathsf{C}\\
0 & \textrm{otherwise}
\end{array}\right.
\end{align}$$


--

<br>

The linear model is: $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathsf{mean}(y) = \alpha + \beta_2 z_{2} + \beta_3 z_{3}$

--

with group means given by:    
$\mu_{\mathsf{A}}=\alpha$     
$\mu_{\mathsf{B}}=\alpha+\beta_2$ ; $\ \ \ \ \beta_2 = \mu_{\mathsf{B}} - \mu_{\mathsf{A}} =$ the 'effect' of being in group $\mathsf{B}$ rather than group $\mathsf{A}$         
$\mu_{\mathsf{C}}=\alpha+\beta_3$ ; $\ \ \ \ \beta_3 = \mu_{\mathsf{C}} - \mu_{\mathsf{A}} =$ the 'effect' of being in group $\mathsf{C}$ rather than group $\mathsf{A}$ 

--

One level (here, $\mathsf{A}$) is treated as a 'baseline', and the $\beta$ coefficients measure the difference between the baseline and another level. This method of numericising factors is called '**treatment contrasts**'.

---

## What about factors with three levels?

--

What hypotheses can we test with this model? 

--

To begin with, we might ask:    
> ***Are any of the group means different from each other?***

--

The null hypothesis for this question is:    
> $\mathsf{H_0}: \mu_\mathsf{A} = \mu_\mathsf{B} = \mu_\mathsf{C}$ $\ \ \ \ \ {}$ or , equivalently,   
$\mathsf{H_0}: \beta_2 = \beta_3 = 0$.

--

The alternative hypothesis is    
> ***at least one pair of means is different***, or equivalently   
***at least one $\beta$ is non-zero***. 

--

How do we test this hypothesis? The Analysis of Variance F-test!

---

## Example: Petrels
.small[

```{r, fig.width=12, fig.height = 3, out.width="60%"}
petrels <- read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/petrels.csv") %>% 
  transmute(`Length of right wing`=R.Wing.Lth, Area=as_factor(Area), Sex=as_factor(Sex)) %>% drop_na()
g_A <- ggplot(petrels, aes(x=Area, y=`Length of right wing`)) + geom_boxplot() 
g_S <- ggplot(petrels, aes(x=Sex, y=`Length of right wing`)) + geom_boxplot() + ylab("")
g_A + g_S + plot_layout(widths=c(3,1))
```
]
---


## Example: Petrels

.left-code-wide[
.small[
```{r echo=F}
mod <- lm(`Length of right wing` ~ Area, data=petrels)
summary(mod)
```
]]

---


## Example: Petrels

.left-code-wide[
.small[
```{r  echo=F, highlight.output=22}
mod <- lm(`Length of right wing` ~ Area, data=petrels)
summary(mod)
```
]]

.right-code[

The P-value for the overall F-test suggests average lengths of right wings do indeed differ among Areas (well, at least one pair of areas have different means).

]

---


## Example: Petrels

.left-code-wide[
.small[
```{r echo=F, highlight.output=11}
mod <- lm(`Length of right wing` ~ Area, data=petrels)
summary(mod)
```
]]

.right-code[


The P-value for the overall F-test suggests average lengths of right wings do indeed differ among Areas (well, at least one pair of areas have different means).

The table of coefficients tells us that birds in Area 1 have wings of length 389.74, on average. This is the baseline for the effects of each of the other Areas.

]

---


## Example: Petrels

.left-code-wide[
.small[
```{r echo=F, highlight.output=12:16}
mod <- lm(`Length of right wing` ~ Area, data=petrels)
summary(mod)
```
]]

.right-code[


The P-value for the overall F-test suggests average lengths of right wings do indeed differ among Areas (well, at least one pair of areas have different means).

The table of coefficients tells us that birds in Area 1 have wings of length 389.74, on average. This is the baseline for the effects of each of the other Areas.

The means of Areas 2-6 were all significantly different to Area 1 (according to the p-values).

]

---


## Example: Petrels

.left-code-wide[
.small[
```{r echo=F, highlight.output=15:16}
mod <- lm(`Length of right wing` ~ Area, data=petrels)
summary(mod)
```
]]

.right-code[

The P-value for the overall F-test suggests average lengths of right wings do indeed differ among Areas (well, at least one pair of areas have different means).

The table of coefficients tells us that birds in Area 1 have wings of length 389.74, on average. This is the baseline for the effects of each of the other Areas.

The means of Areas 2-6 were all significantly different to Area 1 (according to the p-values).

Birds in Area 5 had the longest wings; 4.00 cm longer than Area 1, on average.

Area 6 had the shortest wings; 9.41 cm shorter than Area 1, on average.


]

---

## Example: Petrels

.left-code-wide[
.small[
```{r echo=F}
mod <- lm(`Length of right wing` ~ Area, data=petrels)
summary(mod)
```
]]

.right-code[

The P-value for the overall F-test suggests average lengths of right wings do indeed differ among Areas (well, at least one pair of areas have different means).

The table of coefficients tells us that birds in Area 1 have wings of length 389.74, on average. This is the baseline for the effects of each of the other Areas.

The means of Areas 2-6 were all significantly different to Area 1 (according to the p-values).

Birds in Area 5 had the longest wings; 4.00 cm longer than Area 1, on average.

Area 6 had the shortest wings; 9.41 cm shorter than Area 1, on average.

What are the means of Areas 2-6? 

]

---

## Example: Petrels


.left-code-wide[

We can get estimates of means and confidence intervals with the `predict()` function.

```{r, echo=TRUE}
new_data <- data.frame(Area=factor(1:6))
predict(mod, new_data, interval="confidence") %>% 
  round(.,1)
```
]

--

.right-code[
.small[
```{r fig.dim=c(5,4), out.width="80%"}
library(visreg); visreg(mod, gg=TRUE)
```
]

Blue lines are the means and grey areas are 95% confidence intervals for the means.

Notice that we are less certain about the means of Areas with smaller sample sizes.

]

---

class: middle, inverse

## Multiple factors

---

## What if we have more than one factor?

We include a block of indicator variables for each factor.

The intercept is the mean of the baseline levels of all factors.

The coefficients for other levels give us their 'effects' on the mean relative to the baseline. 

The overall F-test and P-value test whether any of the factors explain variation in $y$.

So, we have the overall F test for the whole model, and effects of individual levels of factors. How can we test for an effect of a factor (including all its levels)?

---

## Linear model summary for a two-factor model

.left-code-wide[
.small[
```{r, echo=TRUE}
summary(lm(`Length of right wing` ~ Sex + Area, petrels))
```
]]


---

## Linear model summary for a two-factor model

.left-code-wide[
.small[
```{r, echo=TRUE, highlight.output=23}
summary(lm(`Length of right wing` ~ Sex + Area, petrels))
```
]]

.right-code[
We have:

- an overall F test for the whole model, inclusive of all factors

]

---

## Linear model summary for a two-factor model

.left-code-wide[
.small[
```{r, echo=TRUE, highlight.output=11}
summary(lm(`Length of right wing` ~ Sex + Area, petrels))
```
]]

.right-code[
We have:

- an overall F test for the whole model, inclusive of all factors

- an estimate of the mean for the baseline levels (females in Area 1)


]


---

## Linear model summary for a two-factor model

.left-code-wide[
.small[
```{r, echo=TRUE, highlight.output=12:17}
summary(lm(`Length of right wing` ~ Sex + Area, petrels))
```
]]

.right-code[
We have:

- an overall F test for the whole model, inclusive of all factors

- an estimate of the mean for the baseline levels (females in Area 1)

- tests of the effects of individual levels of factors relative to the baseline

]


---

## Linear model summary for a two-factor model

.left-code-wide[
.small[
```{r, echo=TRUE}
summary(lm(`Length of right wing` ~ Sex + Area, petrels))
```
]]

.right-code[
We have:

- an overall F test for the whole model, inclusive of all factors

- an estimate of the mean for the baseline levels (females in Area 1)

- tests of the effects of individual levels of factors relative to the baseline

How can we test for an effect of a whole factor, including all its levels (`Age` or `Area`)?

]

---

## `anova()` summary for a two-factor model
.left-code-wide[
.small[
```{r}
anova(lm(`Length of right wing` ~ Sex + Area, data=petrels))
```
]]

--

.right-code[

Each row in the ANOVA table corresponds to one 'term'.

Provides an F-test based on how much variation a term explains relative to how much variation it is expected to explain under a true null hypothesis.

]

---


## Process of assessing a factor

1. Fit the model.

2. Use `anova()` to test the factor as a whole (P-value < 0.05).

3. If it is significant, look at the coefficients table in the `summary()` output for tests of each level relative to the baseline (and use `visreg()` to visualise them).


---

## Visualising the model

```{r fig.dim=c(10,4), out.width="80%"}
library(visreg)
visreg(lm(`Length of right wing` ~ Sex + Area, petrels), 
       "Area", by = "Sex", gg=TRUE)
```

---

## Visualising the model

```{r fig.dim=c(10,4), out.width="80%"}
library(visreg)
visreg(lm(`Length of right wing` ~ Sex + Area, petrels), 
       "Sex", by = "Area", gg=TRUE)
```
---


## The order of terms matters with `anova`

... unless you have a precisely balanced design (e.g., the same number of males and females in each Area). 

.pull-left[
.small[
```{r}
lm(`Length of right wing` ~ Sex + Area, 
   data = petrels) %>% anova
```
]]

.pull-right[
.small[
```{r}
lm(`Length of right wing` ~ Area + Sex, 
   data = petrels) %>% anova
```
]]

--

Think of the total sum of squares (SS, `Sum Sq`) as the whole pie, the variation in $y$ that can potentially be explained by the predictors. The first term gets to explain as much as it can. Subsequent terms only get to explain what is not taken by the preceding terms. 

--

It's first come, first served. This is "Type 1 Sequential" Sums of Squares. There are other types. 

---

## ANOVA table

Each row tells us how much variation in $y$ is explained by each term in the model, and does a significance test.

Each term is conditional on the terms above, but not below.

Order can be important, depending on the correlation and imbalance in the sampling design.

If a factor is signficant, use the summary table and/or plots to see which groups are different.

---

## Interpreting the coefficients table

From ANOVA table, both `Area` and `Sex` affect mean length of the right wing. 

.pull-left[
#### Table of coefficients
.small[
```{r, echo=FALSE}
mod2 = lm(`Length of right wing` ~ Sex + Area, petrels)
summary(mod2)$coef %>% round(.,2)
co <- round(coef(mod2),2)
```
]]

--

.pull-right[
#### Raw differences in means by `Sex`
.small[
```{r, echo=TRUE}
petrels %>% group_by(Sex) %>% 
 summarise(Mean=mean(`Length of right wing`)) %>%
 mutate(`Diff. from Female` = Mean - first(Mean))
```
]]

--

Why is the coefficient for `SexMale` (2.44 mm) not the same as the raw difference between the sexes in the data (3.11 mm)? 

--

The coefficient is an estimate of the difference **when accounting for differences among areas**. Because the ratio of sexes is different among areas, some of the apparent difference between sexes could also be explained by being in different Areas.

---

## Interpreting the coefficients table

From ANOVA table, both `Area` and `Sex` affect mean length of the right wing. 

.pull-left[
#### Table of coefficients
.small[
```{r, echo=FALSE}
mod2 = lm(`Length of right wing` ~ Sex + Area, petrels)
summary(mod2)$coef %>% round(.,2)
co <- round(coef(mod2),2)
```
]]

--

.pull-right[
#### Raw differences in means by `Area`
.small[
```{r, echo=TRUE}
petrels %>% group_by(Area) %>% 
 summarise(Mean=mean(`Length of right wing`)) %>%
 mutate(`Diff. from Area 1` = Mean - first(Mean))
```
]]

--

Similarly, the model estimates that birds in Area 6 have right wings 8.84 mm shorter than those in Area 1 **when accounting for differences in sex**. 

Ignoring sex, they're 9.41 mm shorter.

---

## Summary

The linear model estimates the **mean** of the response variable $y$ given the predictors $x$. For example,    
- the mean body weight for a given heart girth
- the mean right wing length for different areas

--

Linear models provide hypothesis tests and measures of uncertainty in estimates (confidence intervals).    
- The overall F-test tells us whether the model as a whole explains significant variation in $y$.    
- Plotting (e.g. with `visreg`) helps to visualise the effects. Remember to check the diagnostic plots too!

--

For factor (grouping) variables, linear models estimate the mean for each level.     
- The first level is treated as a baseline, and its mean is captured by the intercept term.     
- Each other level is represented by the difference to the baseline.

--

For linear models with multiple factors      
- First, look at the overall F test for the model
- Then, look at the `anova` table to test whole terms
- Then, look at the `summary` table of coefficients to quantify and test differences between the baseline and other levels. Also look at plots.
- Remember that 'not significant' does not mean 'not important'. It means we don't have much evidence that the effect exists, but absence of evidence is not evidence of absence.

