---
title: "Lecture C01"
subtitle: "Simple Linear Regression"
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "part_c.css"]
    # mathjax: "https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, echo=FALSE}
library(broom)
library(knitr)
library(ggplot2); theme_set(theme_bw(base_size=15))
library(patchwork)
opts_chunk$set(dev.args=list(bg='transparent'), comment="", warning=FALSE, echo=FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(8,5), out.width="70%", fig.retina=2)
```

## Learning Outcomes

- The linear model.

- Using R to fit linear models.

- Interpret the R outcomes. 

---

class: middle, inverse

## If I have seen further it is by standing on the shoulders of giants 
  â€” Isaac Newton 

---

## Two centuries of history 

--

The earliest regression was fit using the method of least squares, which was published by Legendre in 1805 and by Gauss in 1809. Both applied the method to the problem of determining the orbits of bodies about the sun from astronomical observations.

--

The term "regression" was coined by Francis Galton in the nineteenth century to describe a biological phenomenon - regression toward the mean. 

--

The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average.
 
--

Galton's work on regression had only this biological meaning and it was later extended by Udny Yule and Karl Pearson to more general statistical contexts.

--

Great mathematicians and statisticians spent almost two hundreds years to build the whole framework for linear regression. We will learn it in two weeks!

---

class: middle, inverse

# Example: 385 Moroccan donkeys

---

## Example: 385 Moroccan donkeys

Variable   | Description
-----------|-----------------
Sex        | Sex (Female/Male)
Age        | Age (years)
Bodywt     | Weight (kg)
Heartgirth | Girth at the heart (cm)
Umbgirth   | Girth at the umbilicus (cm)
Length     | Length from elbow to hind (cm)
Height     | Height to the withers (cm)

---

## Donkeys: relationships between body weight and other measurements

```{r, fig.align="left", echo=TRUE, fig.dim = c(14,3), out.width="100%"}
donkey <- read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
g1 <- ggplot(donkey, aes(x=Heartgirth, y=Bodywt)) + geom_point(col="#7f577492")
g2 <- ggplot(donkey, aes(x=Umbgirth, y=Bodywt)) + geom_point(col="#7f577492")
g3 <- ggplot(donkey, aes(x=Length, y=Bodywt)) + geom_point(col="#7f577492")
g4 <- ggplot(donkey, aes(x=Height, y=Bodywt)) + geom_point(col="#7f577492")
g5 <- ggplot(donkey, aes(x=Sex, y=Bodywt)) + geom_boxplot(col="#7f577492") 
g1 + g2 + g3 + g4 + g5 + plot_layout(nrow=1) 
```

--

Which variables are most strongly related to body weight?

--

Let's build a model of body weight based on its linear relationship with heart girth.

---

class: middle, inverse

# Simple linear regression

---

## Simple linear regression 

--

Simple linear regression is a simplest form of a linear model.

--

The variable of interest, $y$, is called the "response variable". 

--

If $y$ has a straight-line (i.e., "linear") relationship with another variable, $x$, we can capture this relationship with a regression model. 

--

More specifically, we model the *mean of* $y$ as a linear function of $x$. 

$$ \mathsf{Mean}[y] = \alpha + \beta x $$
--

Regression is primarily used for:
- predicting the value of $y$ for a given value of $x$, and 
- quantifying the strength of $y$'s association with $x$.

---


## Simple linear regression - donkeys

For example, $y$ can be the body weight and $x$ the heart girth of donkeys. 

Our model is: *Mean body weight* $= \alpha + \beta \times$*heart girth*

```{r echo=FALSE, fig.width=8, fig.height=6, message = FALSE, out.width="65%"}
model_wt_hg <- lm(Bodywt ~ Heartgirth, data=donkey)
ab <- coef(model_wt_hg)
p <- augment(model_wt_hg, interval='prediction')

g1 <- ggplot(p, aes(x=Heartgirth, y=Bodywt)) +
  ylab("Response variable (Body weight in kg)") +
  xlab("Explanatory variable (Heart girth in cm)") +
  xlim(79,140) +
  ylim(0,229)
g1 + geom_point(col="#7f577492")
```

---

## Simple linear regression - donkeys


For example, $y$ can be the body weight and $x$ the heart girth of donkeys. 

Our model is: *Mean body weight* $= \alpha + \beta \times$*heart girth*

```{r echo=FALSE,  fig.width=8, fig.height=6, message = FALSE, out.width="65%"}
g2 <- g1 + 
  geom_smooth(method="lm", fill = 4) +
  geom_point(col="#7f577492") + 
  annotate('text', x=121, y=70, label= 
             expression("Predicted mean body\nweight given heart girth"), 
           hjust=0) +
  annotate('curve', x=120, y=80, curvature = -0.2, 
           xend = 110, yend = ab[1] + ab[2]*110-2,
           arrow=arrow(angle=20, type='closed', 
                       length=unit(0.15, 'inches')))
g2
```

---

## Simple linear regression - donkeys

For example, $y$ can be the body weight and $x$ the heart girth of donkeys. 

Our model is: *Mean body weight* $= \alpha + \beta \times$*heart girth*

```{r echo=FALSE, fig.width=8, fig.height=6, out.width="65%", message = FALSE}
g3 <- g2 +
  geom_smooth(method="lm", fill = 4) +
  geom_point(col="#7f577492") +
  annotate('text', x=95, y=140, label = expression("Variation in body weights\naround predicted mean,\nor 'residual error'"), hjust=1) +
  geom_ribbon(aes(ymin=.lower, ymax=.upper), alpha=.2) + 
  geom_point(col="#7f577492") +
  annotate('curve', x=95, y=160, curvature = -0.2, 
           xend = 110, yend = ab[1] + ab[2]*110 + 20,
           arrow=arrow(angle=20, type='closed', 
                       length=unit(0.15, 'inches')))
g3
```

---

## Recap and definitions

--

$y$ is the **response**, or **dependent variable**, that we wish to model.

--

$x$ is the **explanatory** variable, **predictor**, **covariate**, or **independent variable**. We use $x$ to explain variation in $y$. When $x$ is numeric, the model is called a "regression", but we can also fit linear model to categorical predictors.

--

The **regression line** tells us the estimate of $y$ for any value of $x$.

--

For simple linear regression, the predicted $y$, or $\hat{y}$, is modelled as:

$$
\hat{y} = \alpha + \beta x
$$
$\alpha$ is the **intercept** or **baseline** estimate of $y$ if $x=0$. 

$\beta$ is the **slope**, or **gradient**, the increase in $y$ for a one-unit increase is $x$. 

---

## Recap and definitions

```{r echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
# generate some data for a generic plot
set.seed(3)
x <- c(runif(20, 0, 5), 2.4)
y <- 0.5 + 0.5*x + rnorm(21, 0, 0.4)
y[21] = y[21] + .2
line <- lm(y ~ x) 
  
simple_plot <- function(x,y) {
  par(mar=c(2,2,1,1), cex=1.5)
  plot(y ~ x, col=1, xlim=c(0, max(x)+0.2), ylim=c(0, max(y)+0.2), 
       pch=19, xlab="", ylab="", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
  line <- lm(y ~ x) 
  abline(line, lwd=2, col=4)
  axis(1, line=0)
}

simple_plot(x,y)
text(4, 2.65, expression(y==alpha+beta*x))
```

---

## Recap and definitions

```{r echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
simple_plot(x,y)
abline(h=coef(line)[1])
text(4, 2.65, expression(y==alpha+beta*x))
xv <- c(1,4)
yv <- predict(line, data.frame(x=xv))
mtext(expression(alpha), side=2, line=0.5, at = coef(line)[1], las=1, cex=1.5)
```

---

## Recap and definitions

```{r echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
simple_plot(x,y)
abline(h=coef(line)[1])
text(4, 2.65, expression(y==alpha+beta*x))
xv <- c(1,4)
yv <- predict(line, data.frame(x=xv))
lines(xv, rep(yv[1], 2), lty="dotted")
lines(rep(xv[2], 2), yv, lty="dotted")
text(mean(xv), yv[1], "run", adj=c(0.5,1.2), col="grey30")
text(xv[2], mean(yv), "rise", adj=c(-0.2,0.5), col="grey30")
mtext(expression(alpha), side=2, line=0.5, at = coef(line)[1], las=1, cex=1.5)
text(4.32, yv[1]+0.2, expression(beta==over(rise,run)))
```

---

## Recap and definitions

```{r echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
simple_plot(x,y)
abline(h=coef(line)[1])
text(4, 2.65, expression(y==alpha+beta*x))
text(4.32, yv[1]+0.2, expression(beta==over(rise,run)))
mtext(expression(alpha), side=2, line=0.5, at = coef(line)[1], las=1, cex=1.5)
xv <- c(1,4)
yf <- function(x) predict(line, data.frame(x=x))

lines(x=0:1, yf(c(0,0)), col="grey30")
lines(x=c(1,1), yf(c(0,1)))
text(.5, yf(.5), "1", adj=c(0.5,3), col="grey30")
text(1.1, yf(.5), expression(beta))

```

---

## Recap and definitions

```{r echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
simple_plot(x,y)
abline(h=coef(line)[1])
text(4, 2.65, expression(y==alpha+beta*x))
text(4.32, yv[1]+0.2, expression(beta==over(rise,run)))
mtext(expression(alpha), side=2, line=0.5, at = coef(line)[1], las=1, cex=1.5)
xv <- c(1,4)
yf <- function(x) predict(line, data.frame(x=x))

lines(x=0:1, yf(c(0,0)), col="grey30" )
lines(x=c(1,1), yf(c(0,1)))
text(.5, yf(.5), "1", adj=c(0.5,3), col="grey30")
text(1.1, yf(.5), expression(beta))

lines(x=1:2, yf(c(1,1)), col="grey30" )
lines(x=c(2,2), yf(1:2))
text(1.5, yf(1.5), "1", adj=c(0.5,3), col="grey30")
text(2.1, yf(1.5), expression(beta))

```



---

## Recap and definitions

```{r echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
simple_plot(x,y)
abline(h=coef(line)[1])
text(4, 2.65, expression(y==alpha+beta*x))
text(4.32, yv[1]+0.2, expression(beta==over(rise,run)))
mtext(expression(alpha), side=2, line=0.5, at = coef(line)[1], las=1, cex=1.5)
xv <- c(1,4)
yf <- function(x) predict(line, data.frame(x=x))

lines(x=0:1, yf(c(0,0)), col="grey30" )
lines(x=c(1,1), yf(c(0,1)))
text(.5, yf(.5), "1", adj=c(0.5,3), col="grey30")
text(1.1, yf(.5), expression(beta))

lines(x=1:2, yf(c(1,1)), col="grey30" )
lines(x=c(2,2), yf(1:2))
text(1.5, yf(1.5), "1", adj=c(0.5,3), col="grey30")
text(2.1, yf(1.5), expression(beta))

lines(x=2:3, yf(c(2,2)), col="grey30" )
lines(x=c(3,3), yf(2:3))
text(2.5, yf(2.5), "1", adj=c(0.5,3), col="grey30")
text(3.1, yf(2.5), expression(beta))

lines(x=3:4, yf(c(3,3)), col="grey30" )
lines(x=c(4,4), yf(3:4))
text(3.5, yf(3.5), "1", adj=c(0.5,3), col="grey30")
text(4.1, yf(3.5), expression(beta))
```


---

## Fixed design and random design

--

We usually observe multiple pairs of observations of $y$ and $x$: $(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)$.

--

Notice that only the mean of $y$ is related to $x$ via the line. 

--

There is almost always random variation in the $y$ values around the regression line, but is there also random variation in the covariate $x$?

--

We often treat $x$ as non-random or 'fixed' by the experimenter, and this is called the 'fixed design setting'. If $x$ is treated as a random variable, the model is in the 'random design setting'. 

--

In this course, we assume all the linear models are in the fixed design setting. Therefore, the values of $x$ are not random. 

---

## A mathematical formulation of the linear model

--

We can denote the data set in simple linear regression by $(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)$

--

We model the uncertainty or noisiness of the individual values of $y_i$ around the regression line takes the form of additive random errors denoted by $\varepsilon_i$.

$$
y_1=\alpha+\beta x_1 +\varepsilon_1,
$$
$$
y_2=\alpha+\beta x_2 +\varepsilon_2,
$$
$$
\vdots
$$
$$
y_n=\alpha+\beta x_n +\varepsilon_n.
$$

You will never observe the true errors! But all the randomness comes from them. 

---

## Sample versus population

--

The $\alpha$ and $\beta$ are known as 'coefficients' or 'parameters', which determine the linear relationship between $y$ and $x$ in the population. These values are fixed and unknown. We could only know them with 100% certainty if we measured every horse that ever lived or ever will live.

--

Instead, we collect data from a sample of horses and fit a model to these data to *estimate* the coefficients. We used our sample data to obtain 'point estimates' $\hat\alpha$ and $\hat\beta$.

--

The sample statistics $\hat\alpha$ and $\hat\beta$ are point estimates of the true population parameters $\alpha$ and $\beta$. This process of estimating population parameters with sample statistics is generally referred to as 'statistical inference'.

--

An infinite number of lines is possible, each determined by different values of the coefficients. We need some criterion for choosing our 'best guess' point estimates of the coefficients based on our data set. 

---

## Estimation of parameters

```{r, echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
# generate some data for a generic plot
simple_plot(x,y)
```

---


## Estimation of parameters

```{r, echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
# generate some data for a generic plot

simple_plot(x,y)
x_i <- x[21]; y_i <- y[21]
y_h <- predict(line, data.frame(x=x_i))
lines(c(x_i, x_i), c(-4, y_i), lty='dotted')
lines(c(-4, x_i), c(y_i, y_i), lty='dotted')
axis(1, at=x_i, line=0, labels=expression(x[8]))
axis(2, at=y_i, line=0, labels=expression(y[8]), las=1)

```

---

## Estimation of parameters

```{r, echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
simple_plot(x,y)
x_i <- x[21]; y_i <- y[21]
y_h <- predict(line, data.frame(x=x_i))
lines(c(x_i, x_i), c(-4, y_i), lty='dotted')
lines(c(-4, x_i), c(y_i, y_i), lty='dotted')
axis(1, at=x_i, line=0, labels=expression(x[8]))
axis(2, at=y_i, line=0, labels=expression(y[8]), las=1)
lines(c(-4, x_i), c(y_h, y_h), lty='dotted')
points(x_i, y_h, pch=16, cex=.7, col = 'dark blue')
axis(2, at=c(y_h, y_i), line=0, labels=expression(hat(y)[8], y[8]), las=1)
```

---

## Estimation of parameters

```{r, echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
simple_plot(x,y)
x_i <- x[21]; y_i <- y[21]
y_h <- predict(line, data.frame(x=x_i))
lines(c(x_i, x_i), c(-4, y_i), lty='dotted')
lines(c(-4, x_i), c(y_i, y_i), lty='dotted')
axis(1, at=x_i, line=0, labels=expression(x[8]))
axis(2, at=y_i, line=0, labels=expression(y[8]), las=1)

shape::Arrows(x_i, y_h, x_i, y_i, col=6, 
              arr.adj = 1, arr.length = 0.2, arr.width= 0.2, arr.type="triangle", lwd=1)
lines(c(-4, x_i), c(y_h, y_h), lty='dotted')
axis(2, at=c(y_h, y_i), line=0, labels=expression(hat(y)[8], y[8]), las=1)
points(x_i, y_h, pch=16, cex=.7, col = 'dark blue')
text(x_i, mean(c(y_h, y_i)), col='black', adj=c(1.5, 0.5), expression(hat(epsilon)[8]), cex=1.2)
```

---


## Estimation of parameters

```{r, echo=FALSE, fig.align="center", fig.dim=c(8,5), out.width="70%"}
y_hat <- predict(line, data.frame(x=x))
simple_plot(x,y)
points(x, y_hat, pch=16, cex=.7, col='dark blue')
shape::Arrows(x, y, x, y_hat, col=6, arr.length=0, lwd=1)
legend(legend=c("Observed data", "Predicted values of y","Residuals (observed y - predicted y)"), x = "topleft", bty="n",
       pch=c(19,16,124), pt.cex=c(1,.7, 0.7), col=c(1,"dark blue",6), cex=.7)

```

---

## Estimating the parameters

--

For each value of $x_i$, we have an estimated value of $y_i$, given by $\hat{y}_i = \hat\alpha + \hat\beta x_i$.

--

The difference between the real $y_i$ and fitted $\hat{y}_i$ is the '*residual*' $\hat\varepsilon_i = y_i - \hat{y}_i$. Residuals represent the random errors, random deviations from the mean of $y$ given $x$. 

- Positive residuals correspond to horses that are heavier than expected given their heart girth. 
- Residuals near zero correspond to horses as heavy as expected given their heart girth. 
- Negative residuals correspond to horses that are lighter than expected given their heart girth. 

--

Our estimates of the intercept and slope $\hat\alpha$ and $\hat\beta$ are obtained by finding the values that minimise the variance of the residuals. This is called 'least squares' estimation.

--

The residual variance is given by:

$$
\begin{aligned}
\mathsf{var}(\hat\varepsilon_i) &= \frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i^2\\
&= \frac{1}{n}\sum_{i=1}^n [y_i - (\hat\alpha + \hat\beta x_i)]^2
\end{aligned}
$$

---

## Estimating the parameters

<iframe src="https://shiny.massey.ac.nz/jcmarsha/leastsquares/" height="500" width="1000" style="border: none"></iframe>

---

class: middle, inverse

# Linear models in R

---

## Linear models in R

```{r, echo=TRUE, eval=FALSE}
mod <- lm(Bodywt ~ Heartgirth, data = donkey)
summary(mod)
```

`lm()` is the R function to fit linear models. We will use it extensively!

--

The first argument is `formula = Bodywt ~ Heartgirth`. More generally, we use the simple formula `response ~ predictor` to specify the required simple regression model. 

--

The second argument `data = donkey` specifies the data object where the response and predictor variables can be found.

--

The function `summary()` is a generic tool used to print a summary of the information in an R object.

---

## Linear models in R

<small>

```{r eval=TRUE, echo=TRUE, message=TRUE}
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
summary(mod)
```

</small>

---

## Linear models in R

--

`Call:`
`lm(formula = Bodywt ~ Heartgirth, data = donkey)` 

simply repeats the model you have fitted.

--

`Residuals:` provides you a quick summary on the distribution of your residuals with `min`, `max`, `median` and another two quartiles. 

--

`Coefficients:` summarizes the information on the estimates for the regression coefficients and `Estimate` column gives us the values of our coefficients. We will discuss the columns `Std. Error`, `t value` and `Pr(>|t|)` in Lecture C2.

--

We can also extract just the coefficients with the function `coef()`. 

```{r,echo=TRUE}
ab <- coef(mod); ab
```

--

In our case, the linear model equation is $\mathsf{Body weight} =$ `r round(ab[1],1)` $+$ `r round(ab[2],2)` $\times \mathsf{ Heart girth}$

--

The estimate for `Heartgirth` means that if the heart girth of Horse A is 1 unit larger than that of Horse B, we expect Horse A to be `r round(coef(mod)[2],2)` units heavier than Horse B, on average.

---

## Let's make a plot 

It is a good idea to look at the regression line on a scatter plot to see how well it fits the data. Just feed the coefficients to the function `geom_abline()`.

```{r,echo=TRUE, fig.dim=c(6,3), message=FALSE}
ab <- coef(mod)
ggplot(donkey, aes(x=Heartgirth, y=Bodywt)) + geom_point() + 
  geom_abline(intercept=ab[1], slope=ab[2], col="dodgerblue")
```

---

## Let's make a plot

There are a other ways to show a regression line, such as with the `geom_smooth()` function specifying `method="lm"`. 

```{r,echo=TRUE, fig.dim=c(6,3), message=FALSE}
ggplot(donkey, aes(x=Heartgirth, y=Bodywt)) + geom_point() +
  geom_smooth(method="lm")
```

---

## Let's make a plot

Or you can use the function `visreg()` from the package `visreg` (see Lab C1).

```{r, echo=TRUE, fig.dim=c(6,3)}
library(visreg)
visreg(mod, gg=TRUE)
```

---

## Summary

A simple linear regression model relates one response variable $y$ to one predictor variable $x$ using a straight line.

The position of a simple regression line is determined by two parameters, the intercept ( $\alpha$ ) and the slope ( $\beta$ ). 

The estimates $\hat\alpha$ and $\hat\beta$ are chosen to minimise residual variance. This means that the regression line explains as much of the variation in $y$ as possible.

To fit a linear model in R, we can use `lm()`, and we can then examine the results via `summary()` and `coef()`.

We can overlay a regression line onto a scatter plot of $y$ vs $x$.
