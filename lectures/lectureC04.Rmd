---
title: "Lecture C04"
subtitle: " Residual diagnostics and transformations"
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "part_c.css"]
    # mathjax: "https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---


```{r setup, echo=FALSE}
library(knitr)
library(ggplot2); theme_set(theme_bw(base_size=15))
library(patchwork)
library(fontawesome)
opts_chunk$set(dev.args=list(bg='transparent'), comment="", warning=FALSE, echo=FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(8,5), out.width="70%", fig.retina=2)
```


## Learning Outcomes

- Diagnose departure from assumptions of the linear model with plots of residials

- Apply transformations to data prior to modelling

---

## Recap

- What are the assumptions for linear model? 

- How are regression coefficients estimated from the data?

- The estimator versus an estimate

- Create your own R functions and make its plot

---

class: middle, inverse

## Residual Diagnostics 

---

## Focus

- Analysing residuals of a linear model to check the Guass-Markov assumptions. 

- Use transformations to remedy the problematic issues arising from violating the assumptions.

---

## The relative importance of the linear model assumptions

--

- **Linearity** is important. Is $y$ linearly related to $x$? 

--

- **Independence** of residuals. Were the data collected in a way such that some outcomes ($y$ values) might directly influence or be similar to others? If so, the standard errors might be wrong.

--

- **Normality** of residuals is the least important assumption, because of the Central Limit Theorem. It is more important for prediction intervals than inferences about parameters.

--

- **Equal variance** of residuals across fitted values $\hat{y}$. This is important. Violation of this assumption can lead to incorrect standard errors and inferences. 

---

## Checking the assumptions

--

How do we know if the assumptions are met?

--

We can examine the data and output from the linear model to help us assess whether the assumptions are appropriate for any given case. 

--

However, for any real dataset and model, you may never know for sure. 

--

To check the linear model assumptions, we'll use the standard **diagnostic plots**.

---

## Checking the assumptions

--

The **diagnostic plots** can help us assess linearity, normality, and equal variance. 

--

We usually can't assess the assumption of independence just by examining the diagnostic plots. We need to consider other information, such as how the data were collected.

--

We can also use the diagnostic plots to look for  **outliers** - data points that have unusually large influence on the fit of the model.

--

In R, we can use `plot(model)` to get four standard diagnostic plots:
1. Residuals (versus fitted values) plot
2. Normal Q-Q plot
3. Scale-location plot
4. Leverage plot 

---

## Example: donkey data


```{r echo=TRUE, eval=FALSE}
read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv") %>% 
  lm(Bodywt ~ Heartgirth, data = .) %>%
  plot(., pch=19, col="#00000040")
```



```{r, fig.align='left', fig.width=5, fig.height=4, out.width="65%", echo=FALSE}
donkey <- read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
par(mfrow=c(2,2), mar=c(4,2,2,2)) # this splits the plot window into four panels
plot(mod, pch=19, col="#00000040")
```


---


## Example: donkey data


```{r echo=TRUE, eval=FALSE}
read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv") %>% 
  lm(Bodywt ~ Heartgirth, data = .) %>%
  plot(., pch=19, col="#00000040")
```



```{r, fig.align='left', fig.width=5, fig.height=4, out.width="65%", echo=FALSE}
donkey <- read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
par(mfrow=c(2,2), mar=c(4,2,2,2)) # this splits the plot window into four panels
plot(mod, pch=19, which=1, col="#00000040")
```


---

## Diagnostic plot 1: residuals versus fitted values

.left-plot[
```{r, echo=FALSE, out.width="100%"}
plot(mod, which=1, pch=19, col="#00000040")
```
]

.right-code[ 

We can use this plot to assess:

1. **linearity** - there should be no curvature, and 

2. **equal variance** - the spread should be constant, with not increase from left to right. 

When either of these fail, a log transformation of $y$ and/or $x$ often helps!
]


---

<iframe src="https://shiny.massey.ac.nz/jcmarsha/linearity/" style="border: none" width="100%" height="600px"></iframe>

https://shiny.massey.ac.nz/jcmarsha/linearity/ 

---

## Diagnostic plot 2: Normal Q-Q plot

.left-plot[

```{r, echo=FALSE, out.width="100%"}
plot(mod, which=2, pch=19, col="#00000040")# , xaxt="n")
```
]

.right-code[ 
We use this plot to assess **normality** of residuals.

Ideally the points will lie on the straight line.

Some departure from the line, particularly at the ends, is no big deal.

The Central Limit Theorem means this can generally be ignored.
]

---

## Diagnostic plot 3: Scale-location plot

.left-plot[

```{r, echo=FALSE, out.width="100%"}
plot(mod, which=3, pch=19, col="#00000040")

```
]

.right-code[ 
We use this plot to assess **equal** or **homogeneity of variance**.

Ideally, points should be flat - not increasing or decreasing.

Residuals versus fit often tells you this just as well, but all residuals are made positive here.
]
---

## Diagnostic plot 4: Residuals versus leverage

.left-plot[

```{r, echo=FALSE, out.width="100%"}
plot(mod, which=5, pch=19, col="#00000040")
```
]

.right-code[ 
We use this plot to look for **influential outliers**.

Points should ideally be inside red bands (Cook's distance) at 0.5.

Points outside Cook's distance of 1 have excessive influence.

]

---

## Influential outliers

Points can be outliers in two ways:

1. They can have extreme $x$ values compared to the rest of the data. Such points are said to have high **leverage**.
    
2. They can have extreme $y$ values, given their $x$ value (i.e. a large residual.)

Points have large **influence** if they exhibit both these properties.

**Cook's distance** is a measure of the influence of a data point on the parameter estimates. Cook's distance larger than 1 means the points have large influence on the model fit. Removing these points would change the model.

---

<iframe src="https://shiny.massey.ac.nz/jcmarsha/influence/" style="border: none" width="100%" height="500px"></iframe>

https://shiny.massey.ac.nz/jcmarsha/influence/

---

## Implication of outliers

The challenge of outliers is that they can significantly distort the results, and 'swamp' the contribution of the majority of the data.

We have seen the outliers in a boxplot and having them often has a significant effect on our mean and standard deviation. Similarly, outliers in our linear model will generally distort the regression coefficient estimates and prediction results. 

You may choose to remove outliers, but it must be reported and justified in your write up. 

---

## Implication of outliers

Be careful though! The outliers may be very informative, perhaps even lead us to new scientific discoveries!

> '*Scientific measurements are never perfect, but different teams studying the same phenomena can often produce widely different results. These “outlier” values are the cause of much consternation – but they may also be a sign of healthy scientific progress*'

Bailey, David (2018) “Why OUTLIERS are good for science”, *Significance*, 15(1): 14-19. https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2018.01105.x

---

## Donkeys: Residuals *vs* fitted values

.left-plot[

```{r, echo=FALSE, out.width="100%"}
plot(mod, which=1, pch=19, col="#00000040")
```
]

--

.right-code[ 

Linearity doesn't hold. There is a curve.

Equal variance doesn't hold. The residuals are increasingly spread out to the right of the plot.

]

---

class: middle, inverse 

## Transformations

---

## Donkeys: Original scale

```{r,fig.align='center', fig.width=8, fig.height=5}
ggplot(donkey, aes(x=Heartgirth, y=Bodywt)) + geom_point()
```

---

## Donkeys: Log scale

```{r,fig.align='center', fig.width=8, fig.height=5}
ggplot(donkey, aes(x=log(Heartgirth), y=log(Bodywt))) + geom_point()
```

---

## Fit log scale model and plot

```{r, fig.align='center', fig.width=6, fig.height=4}
mod2 <- lm(log(Bodywt) ~ log(Heartgirth), data=donkey)
plot(mod2, which=1, pch=19, col="#00000040")
```

Linearity and equal variance now hold.

---

## What have we done?

- We transformed both $x$ and $y$ and then fit a straight line.

- This is the same as fitting a curve to the original data (a power curve).

- We can visualise the model using the `visreg` package. 

---

## Visualising the model -- linear on the log scale

```{r, fig.align='center', fig.width=8, fig.height=5}
library(visreg)
visreg(mod2, xtrans=log, partial=TRUE, gg=TRUE) + ylab("log(Bodywt)") + xlab("log(Bodywt)")
```

---

## Visualising the model -- curved on the original scale

```{r, fig.align='center', fig.width=8, fig.height=5}
library(visreg)
visreg(mod2, trans=exp, partial=TRUE, gg=TRUE) + ylab("Bodywt")
```

---

## Inverse transformation

We have seen the power of log transformation which provides us an efficient remediation to **non-linearity** and **heteroskedasticity** (i.e. such a wordy statistical terminology which means **unequal variances**.)

Notice that we take the log transformation for both `Bodywt` ( $y$ ) and `Heartgirth` ( $x$ ). After a log transformation, the linear model becomes
$$
\log(y) = \alpha + \beta \log(x) +\varepsilon
$$

Exponentiating both sides leads to 
$$
y=e^\alpha\cdot x^\beta \cdot e^\varepsilon
$$

which implies a power law between `Bodywt` and `Heartgirth` with a multiplicative error term $e^\varepsilon$.      
$e^\varepsilon$ follows a log-normal distribution. 

---

## More non-linear transformations

--

The exponential function is the inverse of the log function. $e^{log(x)} = x$

--

Sometimes, we take the log of only $x$ 

$$y = \alpha + \beta \log(x) +\varepsilon$$
or only $y$
$$\log(y) = \alpha + \beta x + \varepsilon$$

--

They both imply a log or exponential law between $y$ and $x$. 

--

The log transformation allows various non-linear functions for the relationship between $x$ and $y$ in bivariate data.

--

The key is to transform the original data in such a way that the relationship between $y'$ and $x'$ becomes linear.

---

## Box-Cox tranformation

The log transformation is powerful, but it is not a panacea!

Box and Cox proposed the following famous transformation named after their last names

$$
y^{(\lambda )} = 
\begin{cases}
{\dfrac {y^{\lambda }-1}{\lambda }} & {\text{if }\lambda \neq 0} \newline
{\log(y)} & {{\text{if }}\lambda =0}
\end{cases}
$$

A bit calculus shows that ${(y^{\lambda}-1)}/{\lambda}\to \log(y)$ as $\lambda\to 0$.

A Box-Cox transformation is a way to transform non-normal variables into a normal shape. 

Notice that it is mainly used to transform **positive** dependent variables (will fail with any $y\le 0$.)

---

## Box-Cox tranformation in R

To use the Box-Cox tranformation in R, we have to load the package `MASS` first.

We then fit a linear model with `lm()` under the original scale and store the results in the object `mod`.

Passing `mod` to the function `boxcox()` will compute an optimal $\lambda$ for your box cox transformation 

```{r, echo=TRUE, eval=FALSE}
library(MASS)
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
boxcox(mod)
```

`boxcox()` returns you a graphical way to choose the most suitable $\lambda$.

---

## Let's have a try

We test the box cox tranformation on the linear model for `Bodywt ~ Heartgirth` under the original scale.


```{r, echo=FALSE, fig.align='center', fig.width=7, fig.height=5}
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
MASS::boxcox(mod)
```



---

## Let's have a try

The middle dash line specifies the best possible value for $\lambda$ while any values between the left and right dash lines would be reasonable. 

So we can pick $\lambda=0$, i.e. a log transformation on $y$, for a neat expression.

--

**Task**: Box-coxing different candidate models with the following codes
```{r, echo=TRUE, eval=FALSE}
mod2 <- lm(log(Bodywt) ~ log(Heartgirth), data=donkey)
boxcox(mod2)
mod3 <- lm(Bodywt ~ log(Heartgirth), data=donkey)
boxcox(mod3)
```

Specify a neat expression for your model from each box-cox plot.

---

## Summary

- Residual diagnostics 

  - The residuals vs fitted plot should be looked at first. If you see curvature or increasing scatter from left to right, try a log transform and re-fit the model.

  - Next look at residuals vs leverage. If you have influential outliers then look at the effect of removing those observations.

- Transformations
    
  - log
    
  - Box-Cox
  